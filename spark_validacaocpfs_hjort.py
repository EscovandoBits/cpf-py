# -*- coding: utf-8 -*-
"""Spark-ValidacaoCPFs-Hjort.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qotmO1ZczgMRiWDlnMTxgh1nvfqjDyWC

# Validação de CPFs - Experimento de validação paralelizada com Spark

https://colab.research.google.com/drive/1qotmO1ZczgMRiWDlnMTxgh1nvfqjDyWC?usp=sharing

# Preparo dos dados

## Geração de números aleatórios
"""

import numpy as np

# Geração de números inteiros com até 9 dígitos
# para posterior cálculo de DV usando algoritmo
# de validação de CPFs
np.random.seed(42)

# quantidade de números a serem gerados
#num_amostras = 200
#num_amostras = int(10e6)
num_amostras = int(100e6)

# Geração das amostras (números inteiros com até 11 dígitos)
amostras = np.random.randint(99999999999, size=num_amostras)

# exibir informações do array de amostra
print('Amostra de valores:\n', amostras)
print('Quantidade de itens:', len(amostras))
print('Tamanho em memória: %.1f MB' % (amostras.size * amostras.itemsize / 1024 / 1024))

"""## Geração de múltiplos arquivos"""

# quantidade de partições (arquivos a serem gerados)
qtd_particoes = 20

# dividir o vetor nas diversas partições
amostras_particoes = np.array_split(amostras, qtd_particoes)
amostras_particoes[0][:10]

!rm -rf arquivos && mkdir arquivos

# gerar um arquivo de texto para cada partição
for i in range(len(amostras_particoes)):
  arq = "arquivos/%03d.txt" % (i+1)
  print(arq)
  np.savetxt(arq, amostras_particoes[i], fmt='%d')

!ls -lah arquivos/*

del amostras_particoes

"""# Criação da função de validação

## Função de validação de dígito verificador de CPF

> Retorna True caso o número de CPF seja válido, False caso contrário.
"""

# Baseado no algoritmo em Linguagem C:
# https://github.com/EscovandoBits/cpf/blob/main/cpf.c

# verifica se um número de CPF é válido
def cpf_valido(n):
  #print('cpf_valido(%011d)' % n)

  # extrair dígitos verificadores
  dv = n % 100
  d10 = dv // 10
  d11 = dv % 10

  # calcular penúltimo dígito
  v1 = 0
  r = n // 100
  i = 9
  while True:
    d = r % 10
    r = r // 10
    v1 += i * d
    i = i - 1
    if not (r > 0 and i > 0):
      break
  v1 = (v1 % 11) % 10
  if (v1 != d10):
    return False

  # calcular último dígito
  v2 = 0
  r = n // 100
  i = 8
  while True:
    d = r % 10
    r = r // 10
    v2 += i * d
    i = i - 1
    if not (r > 0 and i > 0):
      break
  v2 += 9 * v1
  v2 = (v2 % 11) % 10
  if (v2 != d11):
    return False

  return True

# teste de execução da função
for num in [11111111111, 11111111112, 22222222222, 22222222221, 123]:
  print('cpf_valido(%011d)? %s' % (num, cpf_valido(num)))

"""# Processamento parelizado

## Instalação do PySpark
"""

#!pip install -q pyspark

"""## Inicialização da sessão Spark"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master('local[*]') \
    .config("spark.driver.memory", "10g") \
    .appName('my-cool-app') \
    .getOrCreate()
#spark = SparkSession.builder.getOrCreate()
spark

"""Referências:
- https://spark.apache.org/docs/latest/rdd-programming-guide.html
- https://stackoverflow.com/questions/32336915/pyspark-java-lang-outofmemoryerror-java-heap-space

## Criação dos dados

### Via vetor na memória
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # criação do vetor de forma distribuída a partir da memória
# rdd = spark.sparkContext.parallelize(amostras)
# #rdd = spark.sparkContext.parallelize(amostras, 10)
# rdd

rdd.getNumPartitions()

rdd.take(10)
#rdd.sample(withReplacement=False, fraction=0.1).take(10)

"""### Via arquivos de texto"""

# criação do vetor a partir dos arquivos de forma preguiçosa
rdd = spark.sparkContext.textFile("arquivos/*.txt").map(lambda x: int(x))
#rdd = spark.read.text("arquivos/*.txt").rdd
rdd

rdd.getNumPartitions()

rdd.take(10)

"""## Filtragem com _filter_"""

rdd.filter(lambda n: n % 7 == 0).take(5)

rdd.filter(lambda x: cpf_valido(x)).take(5)

#del amostras

# Commented out IPython magic to ensure Python compatibility.
# %%time
# cpfs_validos = rdd.filter(lambda x: cpf_valido(x)).collect()

"""## Resultado"""

# exibição do resultado
cpfs_validos[:10]

print("Do total de %d números da amostra, apenas %d são CPFs válidos (%.2f%%)." % (
    rdd.count(), len(cpfs_validos),
    len(cpfs_validos) / rdd.count() * 100))

"""## Finalização da sessão Spark"""

spark.stop()